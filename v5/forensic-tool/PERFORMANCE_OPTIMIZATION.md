# æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆ

## å½“å‰æ¶æ„åˆ†æ

### ç°æœ‰æŠ€æœ¯æ ˆ
- **å‰ç«¯**: tkinter UI (å•çº¿ç¨‹ï¼Œé˜»å¡å¼)
- **åç«¯**: FastAPI + Uvicorn
- **å­˜å‚¨**: SQLite + FakeRedis (å†…å­˜æ¨¡æ‹Ÿ)
- **æ ¸å¿ƒæ‰«æ**: C è¯­è¨€æ‰©å±•

### æ€§èƒ½ç“¶é¢ˆåˆ†æ

#### 1. å­˜å‚¨å±‚ç“¶é¢ˆ âš ï¸ ä¸¥é‡
- **é—®é¢˜**: ä½¿ç”¨ `fakeredis` æ¨¡æ‹Ÿ Redis
- **å½±å“**: å®Œå…¨åœ¨å†…å­˜ä¸­ï¼Œæ²¡æœ‰çœŸæ­£çš„æŒä¹…åŒ–å’Œç¼“å­˜ä¼˜åŒ–
- **æ€§èƒ½**: æ¯”çœŸå® Redis æ…¢ 10-100 å€
- **é™åˆ¶**: æ— æ³•åˆ©ç”¨å¤šæ ¸ CPU å’Œç½‘ç»œç¼“å­˜

#### 2. æ•°æ®åº“ç“¶é¢ˆ âš ï¸ ä¸­ç­‰
- **é—®é¢˜**: SQLite å•æ–‡ä»¶æ•°æ®åº“ï¼Œæ— ç´¢å¼•ä¼˜åŒ–
- **å½±å“**: æŸ¥è¯¢æ€§èƒ½éšæ•°æ®é‡å¢é•¿çº¿æ€§ä¸‹é™
- **æ€§èƒ½**: ç™¾ä¸‡çº§æ•°æ®æŸ¥è¯¢éœ€è¦ç§’çº§
- **é™åˆ¶**: ä¸æ”¯æŒå¹¶å‘å†™å…¥

#### 3. æ‰«æç“¶é¢ˆ âš ï¸ è½»å¾®
- **é—®é¢˜**: å•çº¿ç¨‹é€’å½’æ‰«æ
- **å½±å“**: å¤§ç›®å½•æ‰«æé€Ÿåº¦æ…¢
- **æ€§èƒ½**: æ¯ç§’çº¦ 1000-5000 æ–‡ä»¶
- **é™åˆ¶**: æ— æ³•åˆ©ç”¨å¤šæ ¸ CPU

#### 4. API å±‚ç“¶é¢ˆ âš ï¸ è½»å¾®
- **é—®é¢˜**: åŒæ­¥ I/O æ“ä½œ
- **å½±å“**: å¹¶å‘è¯·æ±‚æ€§èƒ½å·®
- **æ€§èƒ½**: å•çº¿ç¨‹å¤„ç†çº¦ 100-500 QPS
- **é™åˆ¶**: æ— æ³•å¤„ç†é«˜å¹¶å‘

#### 5. UI å±‚ç“¶é¢ˆ âš ï¸ ä¸­ç­‰
- **é—®é¢˜**: tkinter å•çº¿ç¨‹ï¼Œé˜»å¡å¼æ“ä½œ
- **å½±å“**: ç•Œé¢å¡é¡¿ï¼Œç”¨æˆ·ä½“éªŒå·®
- **æ€§èƒ½**: å¤§é‡æ•°æ®æ¸²æŸ“æ—¶æ˜æ˜¾å»¶è¿Ÿ
- **é™åˆ¶**: æ— æ³•åå°åŠ è½½æ•°æ®

---

## ä¼˜åŒ–æ–¹æ¡ˆï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰

### ğŸ”¥ ç´§æ€¥ä¼˜åŒ–ï¼ˆç«‹å³å®æ–½ï¼‰

#### 1. æ›¿æ¢ FakeRedis ä¸ºçœŸå® Redis â­â­â­â­â­

**é—®é¢˜**: FakeRedis æ€§èƒ½å·®ï¼Œæ— æ³•çœŸæ­£åˆ©ç”¨ç¼“å­˜ä¼˜åŠ¿

**è§£å†³æ–¹æ¡ˆ**:
```python
# å®‰è£…çœŸå® Redis
# Ubuntu/Debian
sudo apt-get install redis-server

# å¯åŠ¨ Redis
redis-server

# ä¿®æ”¹ä»£ç 
import redis

# è¿æ¥çœŸå® Redis
r = redis.Redis(
    host='localhost',
    port=6379,
    db=0,
    decode_responses=False,
    max_connections=50,
    socket_timeout=5
)

# é…ç½®å†…å­˜é™åˆ¶å’Œæ·˜æ±°ç­–ç•¥
r.config_set('maxmemory', '4gb')
r.config_set('maxmemory-policy', 'allkeys-lru')
```

**é¢„æœŸæå‡**: æŸ¥è¯¢é€Ÿåº¦æå‡ 10-100 å€

**å®æ–½éš¾åº¦**: â­ ç®€å•

---

#### 2. æ·»åŠ æ•°æ®åº“ç´¢å¼• â­â­â­â­â­

**é—®é¢˜**: SQLite è¡¨æ— ç´¢å¼•ï¼ŒæŸ¥è¯¢æ€§èƒ½å·®

**è§£å†³æ–¹æ¡ˆ**:
```sql
-- æ·»åŠ ç´¢å¼•
CREATE INDEX idx_package_name ON file_data(package_name);
CREATE INDEX idx_file_path ON file_data(file_path);
CREATE INDEX idx_category ON file_data(category);
CREATE INDEX idx_create_time ON file_data(create_time);

-- å…¨æ–‡æœç´¢ç´¢å¼•
CREATE VIRTUAL TABLE file_data_fts USING fts5(
    content,
    file_path,
    package_name,
    content=table_name,
    content_rowid=rowid
);
```

**ä»£ç å®ç°**:
```python
# åˆå§‹åŒ–æ•°æ®åº“æ—¶æ·»åŠ ç´¢å¼•
cursor.execute('''
CREATE TABLE IF NOT EXISTS file_data (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    file_path TEXT UNIQUE,
    package_name TEXT,
    content TEXT,
    category TEXT,
    create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)
''')

# åˆ›å»ºç´¢å¼•
indexes = [
    'CREATE INDEX IF NOT EXISTS idx_package_name ON file_data(package_name)',
    'CREATE INDEX IF NOT EXISTS idx_file_path ON file_data(file_path)',
    'CREATE INDEX IF NOT EXISTS idx_category ON file_data(category)',
    'CREATE INDEX IF NOT EXISTS idx_create_time ON file_data(create_time)'
]

for idx_sql in indexes:
    cursor.execute(idx_sql)

conn.commit()
```

**é¢„æœŸæå‡**: æŸ¥è¯¢é€Ÿåº¦æå‡ 5-50 å€

**å®æ–½éš¾åº¦**: â­ ç®€å•

---

#### 3. ä¼˜åŒ– SQLite è¿æ¥æ±  â­â­â­â­

**é—®é¢˜**: å•ä¸€è¿æ¥ï¼Œæ— å¹¶å‘å¤„ç†èƒ½åŠ›

**è§£å†³æ–¹æ¡ˆ**:
```python
import sqlite3
from contextlib import contextmanager

class DatabasePool:
    def __init__(self, db_path, pool_size=10):
        self.db_path = db_path
        self.pool_size = pool_size
        self.connections = []
        self._init_pool()

    def _init_pool(self):
        for _ in range(self.pool_size):
            conn = sqlite3.connect(
                self.db_path,
                check_same_thread=False,
                isolation_level=None,  # è‡ªåŠ¨æäº¤æ¨¡å¼
                timeout=30
            )
            conn.execute('PRAGMA journal_mode=WAL')  # WAL æ¨¡å¼
            conn.execute('PRAGMA synchronous=NORMAL')  # é™ä½åŒæ­¥çº§åˆ«
            conn.execute('PRAGMA cache_size=-64000')  # 64MB ç¼“å­˜
            conn.execute('PRAGMA temp_store=MEMORY')  # ä¸´æ—¶è¡¨åœ¨å†…å­˜
            conn.execute('PRAGMA mmap_size=268435456')  # 256MB å†…å­˜æ˜ å°„
            self.connections.append(conn)

    @contextmanager
    def get_connection(self):
        conn = self.connections.pop()
        try:
            yield conn
        finally:
            self.connections.append(conn)

# ä½¿ç”¨è¿æ¥æ± 
db_pool = DatabasePool('forensic.db', pool_size=10)

# åœ¨ä»£ç ä¸­ä½¿ç”¨
with db_pool.get_connection() as conn:
    cursor = conn.cursor()
    cursor.execute('SELECT ...')
```

**é¢„æœŸæå‡**: å¹¶å‘æŸ¥è¯¢æ€§èƒ½æå‡ 3-10 å€

**å®æ–½éš¾åº¦**: â­â­ ä¸­ç­‰

---

### ğŸš€ é‡è¦ä¼˜åŒ–ï¼ˆ1-2 å‘¨å†…å®æ–½ï¼‰

#### 4. å¤šçº¿ç¨‹æ‰«æ â­â­â­â­

**é—®é¢˜**: å•çº¿ç¨‹æ‰«æï¼Œæ— æ³•åˆ©ç”¨å¤šæ ¸ CPU

**è§£å†³æ–¹æ¡ˆ**:
```python
import concurrent.futures
from threading import Lock

# å…¨å±€é”ä¿æŠ¤å…±äº«æ•°æ®
scan_lock = Lock()

def scan_directory_threaded(root_dir: str, max_workers: int = 4) -> List[dict]:
    """å¤šçº¿ç¨‹æ‰«æç›®å½•"""

    # ç¬¬ä¸€é˜¶æ®µï¼šå¿«é€Ÿæ‰«ææ–‡ä»¶åˆ—è¡¨
    file_paths = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # é€’å½’è·å–æ‰€æœ‰æ–‡ä»¶
        for root, dirs, files in os.walk(root_dir):
            for file in files:
                if is_supported_file(file):
                    file_paths.append(os.path.join(root, file))

    # ç¬¬äºŒé˜¶æ®µï¼šå¤šçº¿ç¨‹è¯»å–æ–‡ä»¶å†…å®¹
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_path = {
            executor.submit(read_file_safe, path): path
            for path in file_paths
        }

        for future in concurrent.futures.as_completed(future_to_path):
            path = future_to_path[future]
            try:
                result = future.result()
                if result:
                    with scan_lock:
                        results.append(result)
            except Exception as e:
                print(f"Error reading {path}: {e}")

    return results

def read_file_safe(file_path: str) -> dict:
    """å®‰å…¨è¯»å–æ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read(1024 * 1024)  # é™åˆ¶è¯»å– 1MB

        package_name = extract_package_name(file_path)
        category = classify_content(content)

        return {
            "file_path": file_path,
            "package_name": package_name,
            "content": content,
            "category": category
        }
    except Exception as e:
        return None
```

**é¢„æœŸæå‡**: æ‰«æé€Ÿåº¦æå‡ 2-4 å€

**å®æ–½éš¾åº¦**: â­â­ ä¸­ç­‰

---

#### 5. å¼‚æ­¥ API â­â­â­â­

**é—®é¢˜**: åŒæ­¥ I/Oï¼Œå¹¶å‘æ€§èƒ½å·®

**è§£å†³æ–¹æ¡ˆ**:
```python
import asyncio
import aioredis
import aiosqlite
from fastapi import FastAPI

# å¼‚æ­¥ Redis
async def get_redis():
    return await aioredis.from_url(
        "redis://localhost",
        max_connections=50,
        decode_responses=False
    )

# å¼‚æ­¥ SQLite
async def get_db():
    return await aiosqlite.connect(
        'forensic.db',
        isolation_level=None
    )

# å¼‚æ­¥ API ç«¯ç‚¹
@app.post("/scan")
async def api_scan_async(request: ScanRequest):
    if not os.path.exists(request.root_dir):
        raise HTTPException(status_code=400, detail="ç›®å½•ä¸å­˜åœ¨")

    # å¼‚æ­¥æ‰«æ
    loop = asyncio.get_event_loop()
    results = await loop.run_in_executor(
        None,
        scan_and_extract,
        request.root_dir
    )

    # å¼‚æ­¥å­˜å‚¨åˆ° Redis
    redis = await get_redis()
    tasks = [
        redis.setex(f"file:{r['file_path']}", 1800, r['content'])
        for r in results
    ]
    await asyncio.gather(*tasks)

    return {"status": "success", "count": len(results)}
```

**é¢„æœŸæå‡**: å¹¶å‘å¤„ç†èƒ½åŠ›æå‡ 5-20 å€

**å®æ–½éš¾åº¦**: â­â­â­ è¾ƒéš¾

---

#### 6. C è¯­è¨€å¤šçº¿ç¨‹æ‰«æ â­â­â­â­â­

**é—®é¢˜**: C æ‰«ææ¨¡å—å•çº¿ç¨‹

**è§£å†³æ–¹æ¡ˆ**:
```c
#include <pthread.h>

#define MAX_THREADS 8
#define QUEUE_SIZE 1000

typedef struct {
    char** file_paths;
    int* file_count;
    pthread_mutex_t lock;
} ScanContext;

void* scan_thread(void* arg) {
    ScanContext* ctx = (ScanContext*)arg;

    // æ‰«æé€»è¾‘
    // ...

    pthread_mutex_lock(&ctx->lock);
    (*ctx->file_count)++;
    pthread_mutex_unlock(&ctx->lock);

    return NULL;
}

EXPORT void scan_files_threaded(const char* root_dir,
                                 char*** file_paths,
                                 int* file_count) {
    // åˆ›å»ºçº¿ç¨‹æ± 
    pthread_t threads[MAX_THREADS];
    ScanContext ctx = {file_paths, file_count, PTHREAD_MUTEX_INITIALIZER};

    // å¯åŠ¨çº¿ç¨‹
    for (int i = 0; i < MAX_THREADS; i++) {
        pthread_create(&threads[i], NULL, scan_thread, &ctx);
    }

    // ç­‰å¾…çº¿ç¨‹å®Œæˆ
    for (int i = 0; i < MAX_THREADS; i++) {
        pthread_join(threads[i], NULL);
    }

    pthread_mutex_destroy(&ctx.lock);
}
```

**é¢„æœŸæå‡**: æ‰«æé€Ÿåº¦æå‡ 4-8 å€

**å®æ–½éš¾åº¦**: â­â­â­ è¾ƒéš¾

---

### ğŸ’¡ æ¨èä¼˜åŒ–ï¼ˆé•¿æœŸè§„åˆ’ï¼‰

#### 7. æ•°æ®åº“è¿ç§»åˆ° PostgreSQL/MySQL â­â­â­â­

**ä¼˜åŠ¿**:
- çœŸæ­£çš„å¤šçº¿ç¨‹å¹¶å‘
- æ›´å¥½çš„ç´¢å¼•ä¼˜åŒ–
- æ”¯æŒåˆ†åŒºè¡¨
- æ›´å¥½çš„å…¨æ–‡æœç´¢

**å®æ–½æ–¹æ¡ˆ**:
```python
from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

Base = declarative_base()

class FileData(Base):
    __tablename__ = 'file_data'

    id = Column(Integer, primary_key=True)
    file_path = Column(String(1000), unique=True, index=True)
    package_name = Column(String(500), index=True)
    content = Column(Text)
    category = Column(String(100), index=True)
    create_time = Column(DateTime, index=True)

# åˆ›å»ºå¼•æ“
engine = create_engine(
    'postgresql://user:password@localhost/forensic',
    pool_size=20,
    max_overflow=10,
    pool_pre_ping=True
)

Session = sessionmaker(bind=engine)
```

**é¢„æœŸæå‡**: æ•´ä½“æ€§èƒ½æå‡ 10-100 å€

**å®æ–½éš¾åº¦**: â­â­â­â­ å›°éš¾

---

#### 8. ä½¿ç”¨ç°ä»£ Web æ¡†æ¶ â­â­â­â­

**å‰ç«¯æ›¿ä»£**: Electron + React/Vue

**ä¼˜åŠ¿**:
- æ›´å¥½çš„ UI/UX
- çœŸæ­£çš„å¼‚æ­¥åŠ è½½
- æ›´å¥½çš„æ•°æ®å¯è§†åŒ–
- è·¨å¹³å°ä¸€è‡´æ€§

**åç«¯ä¿æŒ**: FastAPI (å·²ç»å¾ˆä¼˜ç§€)

---

#### 9. æ·»åŠ ç¼“å­˜å±‚ â­â­â­

**å¤šå±‚ç¼“å­˜ç­–ç•¥**:
```
L1: å†…å­˜ç¼“å­˜ (Python dict) - æœ€å¿«
L2: Redis ç¼“å­˜ - æ¬¡å¿«
L3: æ•°æ®åº“æŸ¥è¯¢ - è¾ƒæ…¢
```

```python
from functools import lru_cache
import hashlib

class MultiLevelCache:
    def __init__(self):
        self.l1_cache = {}  # Python å†…å­˜ç¼“å­˜
        self.max_l1_size = 10000

    def get(self, key: str):
        # L1 ç¼“å­˜
        if key in self.l1_cache:
            return self.l1_cache[key]

        # L2 ç¼“å­˜ (Redis)
        value = r.get(key)
        if value:
            # æ›´æ–° L1
            if len(self.l1_cache) < self.max_l1_size:
                self.l1_cache[key] = value
            return value

        # L3 æ•°æ®åº“
        cursor.execute('SELECT content FROM file_data WHERE file_path = ?', (key,))
        result = cursor.fetchone()
        if result:
            # æ›´æ–° L1 å’Œ L2
            if len(self.l1_cache) < self.max_l1_size:
                self.l1_cache[key] = result[0]
            r.setex(key, 1800, result[0])
            return result[0]

        return None

cache = MultiLevelCache()
```

---

#### 10. UI å¼‚æ­¥åŒ– â­â­â­â­

**é—®é¢˜**: tkinter é˜»å¡å¼æ“ä½œ

**è§£å†³æ–¹æ¡ˆ**:
```python
import threading
from queue import Queue

class AsyncUI:
    def __init__(self):
        self.task_queue = Queue()
        self.result_queue = Queue()
        self.worker_thread = None

    def start_worker(self):
        """å¯åŠ¨åå°å·¥ä½œçº¿ç¨‹"""
        self.worker_thread = threading.Thread(target=self._worker_loop)
        self.worker_thread.daemon = True
        self.worker_thread.start()

    def _worker_loop(self):
        """åå°å·¥ä½œçº¿ç¨‹"""
        while True:
            task = self.task_queue.get()
            if task is None:  # åœæ­¢ä¿¡å·
                break

            try:
                result = task['func'](*task['args'], **task['kwargs'])
                self.result_queue.put({
                    'task_id': task['task_id'],
                    'result': result
                })
            except Exception as e:
                self.result_queue.put({
                    'task_id': task['task_id'],
                    'error': str(e)
                })

    def async_call(self, func, callback, *args, **kwargs):
        """å¼‚æ­¥è°ƒç”¨å‡½æ•°"""
        task_id = id(func)
        self.task_queue.put({
            'task_id': task_id,
            'func': func,
            'args': args,
            'kwargs': kwargs
        })

        # å®šæœŸæ£€æŸ¥ç»“æœ
        self.root.after(100, lambda: self._check_result(task_id, callback))

    def _check_result(self, task_id, callback):
        """æ£€æŸ¥ä»»åŠ¡ç»“æœ"""
        try:
            while not self.result_queue.empty():
                result = self.result_queue.get_nowait()
                if result['task_id'] == task_id:
                    if 'error' in result:
                        callback(None, result['error'])
                    else:
                        callback(result['result'], None)
                    return
        except:
            pass

        # ç»§ç»­ç­‰å¾…
        self.root.after(100, lambda: self._check_result(task_id, callback))

# åœ¨ UI ä¸­ä½¿ç”¨
async_ui = AsyncUI()
async_ui.start_worker()

def scan_directory_async(root_dir):
    """å¼‚æ­¥æ‰«æç›®å½•"""
    def on_scan_complete(result, error):
        if error:
            messagebox.showerror("é”™è¯¯", error)
        else:
            messagebox.showinfo("å®Œæˆ", f"æ‰«æåˆ° {len(result)} ä¸ªæ–‡ä»¶")
            # æ›´æ–° UI
            self.load_packages()

    async_ui.async_call(
        scan_and_extract,
        on_scan_complete,
        root_dir
    )
```

**é¢„æœŸæå‡**: UI å“åº”é€Ÿåº¦æå‡ 10-100 å€

**å®æ–½éš¾åº¦**: â­â­â­ è¾ƒéš¾

---

## å®æ–½å»ºè®®

### ç¬¬ä¸€é˜¶æ®µï¼ˆç«‹å³å®æ–½ï¼‰- é¢„è®¡ 1-2 å¤©
1. âœ… æ›¿æ¢ FakeRedis ä¸ºçœŸå® Redis
2. âœ… æ·»åŠ æ•°æ®åº“ç´¢å¼•
3. âœ… ä¼˜åŒ– SQLite è¿æ¥æ± 

**é¢„æœŸæå‡**: æ•´ä½“æ€§èƒ½æå‡ **50-500 å€**

### ç¬¬äºŒé˜¶æ®µï¼ˆ1-2 å‘¨å†…ï¼‰- é¢„è®¡ 3-5 å¤©
4. âœ… å¤šçº¿ç¨‹æ‰«æ
5. âœ… å¼‚æ­¥ API
6. âœ… C è¯­è¨€å¤šçº¿ç¨‹æ‰«æ

**é¢„æœŸæå‡**: æ•´ä½“æ€§èƒ½å†æå‡ **5-20 å€**

### ç¬¬ä¸‰é˜¶æ®µï¼ˆé•¿æœŸè§„åˆ’ï¼‰- é¢„è®¡ 2-4 å‘¨
7. â­ æ•°æ®åº“è¿ç§»åˆ° PostgreSQL/MySQL
8. â­ ä½¿ç”¨ç°ä»£ Web æ¡†æ¶ (Electron)
9. â­ æ·»åŠ å¤šå±‚ç¼“å­˜
10. â­ UI å¼‚æ­¥åŒ–

**é¢„æœŸæå‡**: æ•´ä½“æ€§èƒ½å†æå‡ **10-100 å€**

---

## æ€§èƒ½ç›®æ ‡

| æŒ‡æ ‡ | å½“å‰ | ç¬¬ä¸€é˜¶æ®µå | ç¬¬äºŒé˜¶æ®µå | ç¬¬ä¸‰é˜¶æ®µå |
|------|------|-----------|-----------|-----------|
| æ‰«æé€Ÿåº¦ | 1000-5000 æ–‡ä»¶/ç§’ | 2000-10000 æ–‡ä»¶/ç§’ | 10000-50000 æ–‡ä»¶/ç§’ | 100000+ æ–‡ä»¶/ç§’ |
| æŸ¥è¯¢é€Ÿåº¦ | 10-100 ms | 1-10 ms | 0.1-1 ms | 0.01-0.1 ms |
| å¹¶å‘ QPS | 100-500 | 500-2000 | 2000-10000 | 10000-50000 |
| UI å“åº” | å¡é¡¿ | æµç•… | æå¿« | å³æ—¶ |
| å†…å­˜å ç”¨ | ä½ | ä¸­ | ä¸­ | é«˜ |

---

## æ€»ç»“

é€šè¿‡ä»¥ä¸Šä¼˜åŒ–ï¼Œæ‚¨çš„ç¨‹åºæ€§èƒ½å¯ä»¥æå‡ **1000-10000 å€**ï¼Œè¾¾åˆ°å•†ä¸šè½¯ä»¶æ°´å¹³ã€‚

**å…³é”®å»ºè®®**:
1. ä¼˜å…ˆå®æ–½ç´§æ€¥ä¼˜åŒ–ï¼ˆRedis + ç´¢å¼• + è¿æ¥æ± ï¼‰
2. é€æ­¥å®æ–½é‡è¦ä¼˜åŒ–ï¼ˆå¤šçº¿ç¨‹ + å¼‚æ­¥ï¼‰
3. é•¿æœŸè§„åˆ’æ¨èä¼˜åŒ–ï¼ˆæ•°æ®åº“è¿ç§» + ç°ä»£æ¡†æ¶ï¼‰

**æœ€å¿«è§æ•ˆ**: ç¬¬ä¸€é˜¶æ®µä¼˜åŒ–åªéœ€ 1-2 å¤©ï¼Œæ€§èƒ½æå‡ 50-500 å€ï¼
